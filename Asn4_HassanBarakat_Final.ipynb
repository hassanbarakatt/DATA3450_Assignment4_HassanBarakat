{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Classification\n",
    "\n",
    "For this assignment, you'll need to perform a classification on a dataset, as well as do some prep work on the data. \n",
    "\n",
    "The exact steps of what you need to do are flexible and up to you to some degree, however you should consider some of the important things we've mentioned recently, such as:\n",
    "<ul>\n",
    "<li> Is the target balanced?\n",
    "<li> Are there missing or erroneous values?\n",
    "<li> Are there categorical or numerical features?\n",
    "<li> Is there colinearity?\n",
    "<li> Are there outliers?\n",
    "<li> Should we normalize? \n",
    "<li> Do the distributions of the features give any indication that some may need work? \n",
    "</ul>\n",
    "\n",
    "<b>The target that we are predicting is the loan_status. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data, Cleaning & EDA\n",
    "Here we load the data, check for logical errors (outliers), and analyze the features to determine credit risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "try:\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback for different folder structures if needed\n",
    "    df = pd.read_csv(\"../train.csv\")\n",
    "\n",
    "print(\"Training data shape:\", df.shape)\n",
    "\n",
    "# --- 1. INITIAL INSPECTION & CLEANING ---\n",
    "# Common issue in this dataset: People with Age > 100 (likely typos).\n",
    "\n",
    "# Check max age\n",
    "print(f\"Max age before cleaning: {df['person_age'].max()}\")\n",
    "\n",
    "# Remove outliers: Drop rows where age is greater than 100\n",
    "df = df[df['person_age'] < 100]\n",
    "print(f\"Max age after cleaning: {df['person_age'].max()}\")\n",
    "\n",
    "# Check for collinearity (Correlation Matrix)\n",
    "# The prompt asks: \"Is there colinearity?\"\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Select only numeric columns for correlation\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "sns.heatmap(numeric_df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix - Checking for Collinearity\")\n",
    "plt.show()\n",
    "\n",
    "# --- 2. CREDIT RISK ANALYSIS (Answering the prompt) ---\n",
    "# Prompt asks: \"State if you see a group that appears to be a good credit risk... other than money\"\n",
    "# Let's look at Loan Grade vs Default Rate\n",
    "print(\"\\n--- Credit Risk Analysis ---\")\n",
    "risk_group = df.groupby('loan_grade')['loan_status'].mean().sort_values()\n",
    "print(\"Default Rate by Loan Grade:\")\n",
    "print(risk_group)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=risk_group.index, y=risk_group.values, palette=\"Blues_d\")\n",
    "plt.ylabel(\"Probability of Default\")\n",
    "plt.title(\"Default Rate by Loan Grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training & Pipeline Construction\n",
    "We will split the data, create a preprocessing pipeline (imputation + scaling), and compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. PREPARATION FOR MODELING ---\n",
    "\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "target_col = \"loan_status\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Train/validation split (Stratified because target is imbalanced ~22% default)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 4. PIPELINES ---\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")), # Median is safer for skewed income/loan amounts\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def make_pipeline(classifier):\n",
    "    return Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", classifier),\n",
    "    ])\n",
    "\n",
    "# --- 5. MODEL SELECTION ---\n",
    "\n",
    "models = {\n",
    "    \"Dummy (Baseline)\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "    # Added class_weight='balanced' to Random Forest to help with the imbalance\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "}\n",
    "\n",
    "f1_scores = {}\n",
    "\n",
    "print(\"\\n=== Model comparison on validation set (F1 score) ===\")\n",
    "for name, clf in models.items():\n",
    "    pipe = make_pipeline(clf)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, preds)\n",
    "    f1_scores[name] = f1\n",
    "    print(f\"{name:20s}: F1 = {f1:.4f}\")\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "print(\"\\nBest model by F1:\", best_model_name)\n",
    "\n",
    "# --- 6. FINAL DEPLOYMENT BUILD ---\n",
    "# Fit on FULL dataset so it's ready for the hidden test set\n",
    "best_classifier = models[best_model_name]\n",
    "model = make_pipeline(best_classifier)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"\\nFinal model trained on full dataset and stored in variable `model`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rationale and Data Preparation Steps\n",
    "\n",
    "### 1. Data Cleaning and Outlier Removal\n",
    "* **Outliers:** Upon inspecting the distributions, I found records with `person_age` greater than 100 (e.g., 144 years old). These are clearly data entry errors. I **dropped these rows** to prevent them from skewing the standardization process.\n",
    "* **Missing Values:** I used **Median Imputation** for numeric features (Income, Loan Amount) because these variables are typically right-skewed, making the median a better representation of the center than the mean. I used **Most Frequent** imputation for categorical features.\n",
    "\n",
    "### 2. Feature Analysis (Collinearity & Risk)\n",
    "* **Collinearity:** I generated a correlation matrix. While there is some correlation between `loan_amnt` and `person_income`, it was not high enough (>0.8) to warrant dropping features, especially since Random Forests handle collinearity well.\n",
    "* **Credit Risk Analysis:** The prompt asks to identify good vs. bad credit risks. \n",
    "    * **Good Risk:** Borrowers with **Loan Grade A and B** have historically very low default rates (visible in the bar chart above).\n",
    "    * **Bad Risk:** Borrowers with **Loan Grade D and E** show drastically higher probabilities of default.\n",
    "    * *Conclusion:* `loan_grade` is likely a highly predictive feature.\n",
    "\n",
    "### 3. Modeling Strategy\n",
    "* **Imbalance:** The target `loan_status` is imbalanced (approx 22% default). To address this, I used the F1-score as my primary metric (accuracy can be misleading here) and set `class_weight='balanced'` in the Random Forest model to penalize misclassifying defaults.\n",
    "* **Final Pipeline:** I created a unified pipeline containing the Imputer, Scaler, Encoder, and the Classifier. I fit this pipeline on the **entire** training dataset to maximize the data available for the final \"deployment\" test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Test\n",
    "\n",
    "Replace the green part with whatever you need to transform the fresh data into the format needed for your model to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load the separate test data file.\n",
    "test_path = \"test.csv\"\n",
    "if not os.path.exists(test_path):\n",
    "    # Fallback if file is named differently in your local folder\n",
    "    alt_path = \"test copy.csv\" \n",
    "    if os.path.exists(alt_path):\n",
    "        test_path = alt_path\n",
    "    else:\n",
    "        # Just creating a placeholder if you don't have the file right now\n",
    "        print(\"Test file not found. Please ensure test.csv is in the folder.\")\n",
    "        test_path = None\n",
    "\n",
    "if test_path:\n",
    "    d_test = pd.read_csv(test_path)\n",
    "    \n",
    "    # Make a copy so we do not accidentally modify the original\n",
    "    test_df = d_test.copy()\n",
    "\n",
    "    # Drop the ID column if present\n",
    "    if \"Unnamed: 0\" in test_df.columns:\n",
    "        test_df = test_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    # Separate features and target\n",
    "    if \"loan_status\" not in test_df.columns:\n",
    "        print(\"Warning: 'loan_status' not found in test data. Cannot calculate accuracy.\")\n",
    "    else:\n",
    "        target_col = \"loan_status\"\n",
    "        xtest = test_df.drop(columns=[target_col])\n",
    "        ytest = test_df[target_col]\n",
    "\n",
    "        # Use the previously trained deployment model to generate predictions\n",
    "        # The pipeline handles all encoding/scaling automatically!\n",
    "        prediction_labels = model.predict(xtest)\n",
    "\n",
    "        print(\"\\nDeployment / hold-out test performance:\")\n",
    "        print(\"F1 Score:\", f1_score(ytest, prediction_labels))\n",
    "        print(\"\\nClassification Report:\\n\")\n",
    "        print(classification_report(ytest, prediction_labels))\n",
    "\n",
    "        # Visualizing the results\n",
    "        conf_matrix = confusion_matrix(ytest, prediction_labels)\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.title(\"Confusion Matrix on Test Data\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}